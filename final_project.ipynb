{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_model, set_default\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code adapted from Yann LeCun and Alfredo Canziani 2019 Spring NYU Deep Learning Course\n",
    "set_default()\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IP_0962375</th>\n",
       "      <td>ISIC_0149568</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_0135517</th>\n",
       "      <td>ISIC_0188432</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_7735373</th>\n",
       "      <td>ISIC_0207268</td>\n",
       "      <td>male</td>\n",
       "      <td>55.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_8349964</th>\n",
       "      <td>ISIC_0232101</td>\n",
       "      <td>male</td>\n",
       "      <td>65.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_3232631</th>\n",
       "      <td>ISIC_0247330</td>\n",
       "      <td>female</td>\n",
       "      <td>65.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_7507212</th>\n",
       "      <td>ISIC_9955163</td>\n",
       "      <td>male</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_1165806</th>\n",
       "      <td>ISIC_9963177</td>\n",
       "      <td>male</td>\n",
       "      <td>70.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_7887363</th>\n",
       "      <td>ISIC_9967383</td>\n",
       "      <td>male</td>\n",
       "      <td>60.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_2860540</th>\n",
       "      <td>ISIC_9978107</td>\n",
       "      <td>male</td>\n",
       "      <td>65.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_2516168</th>\n",
       "      <td>ISIC_9998682</td>\n",
       "      <td>male</td>\n",
       "      <td>60.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>584 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_name     sex  age_approx anatom_site_general_challenge  \\\n",
       "patient_id                                                                   \n",
       "IP_0962375  ISIC_0149568  female        55.0               upper extremity   \n",
       "IP_0135517  ISIC_0188432  female        50.0               upper extremity   \n",
       "IP_7735373  ISIC_0207268    male        55.0                         torso   \n",
       "IP_8349964  ISIC_0232101    male        65.0                         torso   \n",
       "IP_3232631  ISIC_0247330  female        65.0               lower extremity   \n",
       "...                  ...     ...         ...                           ...   \n",
       "IP_7507212  ISIC_9955163    male        55.0               upper extremity   \n",
       "IP_1165806  ISIC_9963177    male        70.0                         torso   \n",
       "IP_7887363  ISIC_9967383    male        60.0               upper extremity   \n",
       "IP_2860540  ISIC_9978107    male        65.0               lower extremity   \n",
       "IP_2516168  ISIC_9998682    male        60.0                     head/neck   \n",
       "\n",
       "           diagnosis benign_malignant  target  \n",
       "patient_id                                     \n",
       "IP_0962375  melanoma        malignant       1  \n",
       "IP_0135517  melanoma        malignant       1  \n",
       "IP_7735373  melanoma        malignant       1  \n",
       "IP_8349964  melanoma        malignant       1  \n",
       "IP_3232631  melanoma        malignant       1  \n",
       "...              ...              ...     ...  \n",
       "IP_7507212  melanoma        malignant       1  \n",
       "IP_1165806  melanoma        malignant       1  \n",
       "IP_7887363  melanoma        malignant       1  \n",
       "IP_2860540  melanoma        malignant       1  \n",
       "IP_2516168  melanoma        malignant       1  \n",
       "\n",
       "[584 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "data.set_index(\"patient_id\", inplace=True)\n",
    "data[\"diagnosis\"].value_counts()\n",
    "data[data[\"diagnosis\"] == \"melanoma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/train_folder\", transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=40, shuffle=True)\n",
    "test_dataset = datasets.ImageFolder(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/test_folder\", transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, shuffle=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8353, 0.8314, 0.8157,  ..., 0.8353, 0.8275, 0.8275],\n",
       "        [0.8392, 0.8314, 0.8078,  ..., 0.8549, 0.8196, 0.8275],\n",
       "        [0.8510, 0.8431, 0.8157,  ..., 0.8549, 0.8235, 0.8235],\n",
       "        ...,\n",
       "        [0.7922, 0.7804, 0.7804,  ..., 0.8235, 0.8314, 0.8275],\n",
       "        [0.7882, 0.7922, 0.7843,  ..., 0.8275, 0.8275, 0.8235],\n",
       "        [0.7922, 0.8000, 0.7961,  ..., 0.8353, 0.8353, 0.8235]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = dataloader.dataset.__getitem__(1)[0]\n",
    "thing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected NN and ConvNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3*224*224\n",
    "output_size = 2\n",
    "\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "#         print(x.size())\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = np.abs(output.detach().numpy())\n",
    "test_output[2][-1] = 1.2\n",
    "thing = np.array([np.max(test_output[i]) for i in range(len(test_output))])\n",
    "thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "def train(epoch, model, perm = torch.arange(0,150528).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#         data = dataloader.dataset.__getitem__(0)[0].view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#         data = data.view(-1, 28*28)\n",
    "#         data = data[:, perm]\n",
    "#         data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))\n",
    "            \n",
    "def test(model, perm = torch.arange(0,150528).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#        data = dataloader.dataset.__getitem__(0)[0].view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#         data = data.view(-1, 28*28)\n",
    "#         data = data[:, perm]\n",
    "#         data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        \n",
    "        output_array = np.array([np.max(output.detach().numpy()[i]) for i in range(output.size()[0])])\n",
    "        fpr, tpr, _ = roc_curve(target.detach().numpy(), output_array)\n",
    "        test_auc = auc(fpr, tpr)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)   AUC: ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1125968\n",
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.753076\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.001518\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.000926\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.000171\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.000325\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 2000/2000 (100%)   AUC: (nan%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ConvNet\n",
    "# Training settings \n",
    "n_features = 8 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1204322\n",
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.936367\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.000160\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.000059\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.000415\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.000402\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [400/2000 (20%)]\tLoss: 0.000039\n",
      "Train Epoch: 1 [800/2000 (40%)]\tLoss: 0.000202\n",
      "Train Epoch: 1 [1200/2000 (60%)]\tLoss: 0.000018\n",
      "Train Epoch: 1 [1600/2000 (80%)]\tLoss: 0.000127\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 2 [0/2000 (0%)]\tLoss: 0.000024\n",
      "Train Epoch: 2 [400/2000 (20%)]\tLoss: 0.000023\n",
      "Train Epoch: 2 [800/2000 (40%)]\tLoss: 0.000007\n",
      "Train Epoch: 2 [1200/2000 (60%)]\tLoss: 0.000033\n",
      "Train Epoch: 2 [1600/2000 (80%)]\tLoss: 0.000256\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fully connected network\n",
    "\n",
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1125968\n",
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.693506\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.600084\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.519524\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.453838\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.399861\n",
      "\n",
      "Test set: Average loss: 0.3550, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 0.355048\n",
      "Train Epoch: 1 [400/2000 (20%)]\tLoss: 0.317492\n",
      "Train Epoch: 1 [800/2000 (40%)]\tLoss: 0.285733\n",
      "Train Epoch: 1 [1200/2000 (60%)]\tLoss: 0.258664\n",
      "Train Epoch: 1 [1600/2000 (80%)]\tLoss: 0.235380\n",
      "\n",
      "Test set: Average loss: 0.2152, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 2 [0/2000 (0%)]\tLoss: 0.215221\n",
      "Train Epoch: 2 [400/2000 (20%)]\tLoss: 0.197640\n",
      "Train Epoch: 2 [800/2000 (40%)]\tLoss: 0.182217\n",
      "Train Epoch: 2 [1200/2000 (60%)]\tLoss: 0.168589\n",
      "Train Epoch: 2 [1600/2000 (80%)]\tLoss: 0.156504\n",
      "\n",
      "Test set: Average loss: 0.1457, Accuracy: 2000/2000 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ConvNet\n",
    "# Training settings \n",
    "n_features = 8 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "As a preprocessing step, we will try to run the images through an autoencoder to reduce image noise and use the outputs as inputs for the CNN. This will hopefully lead to increased classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define AutoEncoder Class\n",
    "input_size = 3*224*224\n",
    "output_size = 2\n",
    "d = 500\n",
    "n_feature = 8\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, d),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "#         print(x.size())\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# AE_model = Autoencoder().to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Define model architecture and reconstruction loss\n",
    "\n",
    "# n = 28 x 28 = 784\n",
    "# d = 30  # for standard AE (under-complete hidden layer)\n",
    "# d = 500  # for denoising AE (over-complete hidden layer)\n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(input_size, d),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(d, input_size),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x\n",
    "    \n",
    "AE_model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    AE_model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmeng1/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([40, 150528])) that is different to the input size (torch.Size([40, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (150528) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-7bc25652713b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# ===================forward=====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bad\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# feed <img> (for std AE) or <img_bad> (for denoising AE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# ===================backward====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (150528) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# for data in dataloader:\n",
    "#     img, _ = data\n",
    "#     img = img.to(device)\n",
    "#     img = img.view(img.size(0), -1)\n",
    "#     noise = do(torch.ones(img.shape))\n",
    "#     img_bad = (img * noise).to(device)\n",
    "#     print(img.shape, noise.shape, img_bad.shape)\n",
    "\n",
    "# Train standard or denoising autoencoder (AE)\n",
    "\n",
    "perm = torch.arange(0,150528).long()\n",
    "\n",
    "num_epochs = 1\n",
    "do = nn.Dropout()  # comment out for standard AE\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "        img.requires_grad_()\n",
    "        img = img.view(img.size(0), -1)\n",
    "        noise = do(torch.ones(img.shape))\n",
    "        img_bad = (img * noise).to(device)  # comment out for standard AE\n",
    "        # ===================forward=====================\n",
    "        output = AE_model(img_bad)  # feed <img> (for std AE) or <img_bad> (for denoising AE)\n",
    "        loss = criterion(output, img.data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print(f'epoch [{epoch + 1}/{num_epochs}], loss:{loss.item():.4f}')\n",
    "#     display_images(None, output)  # pass (None, output) for std AE, (img_bad, output) for denoising AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150528"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
