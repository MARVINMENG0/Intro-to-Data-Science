{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_model, set_default\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from skimage.transform import rotate\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import gaussian\n",
    "from skimage.io import imread, imsave\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from torchsummary import summary\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code adapted from Yann LeCun and Alfredo Canziani 2019 Spring NYU Deep Learning Course\n",
    "set_default()\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IP_7279968</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_3075186</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_2842074</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_6890425</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_8723313</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_name     sex  age_approx anatom_site_general_challenge  \\\n",
       "patient_id                                                                   \n",
       "IP_7279968  ISIC_2637011    male        45.0                     head/neck   \n",
       "IP_3075186  ISIC_0015719  female        45.0               upper extremity   \n",
       "IP_2842074  ISIC_0052212  female        50.0               lower extremity   \n",
       "IP_6890425  ISIC_0068279  female        45.0                     head/neck   \n",
       "IP_8723313  ISIC_0074268  female        55.0               upper extremity   \n",
       "\n",
       "           diagnosis benign_malignant  target  \n",
       "patient_id                                     \n",
       "IP_7279968   unknown           benign       0  \n",
       "IP_3075186   unknown           benign       0  \n",
       "IP_2842074     nevus           benign       0  \n",
       "IP_6890425   unknown           benign       0  \n",
       "IP_8723313   unknown           benign       0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "data.set_index(\"patient_id\", inplace=True)\n",
    "data[\"diagnosis\"].value_counts()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "\n",
    "### Code by Andrew Jong https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0][-16:-4]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "dataset = ImageFolderWithPaths(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/train_folder\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/train_folder\", transform=transform)\n",
    "test_dataset = ImageFolderWithPaths(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/test_folder\", transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "Rotating melanoma images to number of samples with melanoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = np.array(dataset.targets) == 1\n",
    "\n",
    "# for i in mel.nonzero()[0]:\n",
    "#     rotated_images_batch = []\n",
    "#     for j in range(1, 41): # range depends on batch size\n",
    "#         rotated_images_batch.append((torch.tensor(rotate(dataloader.dataset.__getitem__(i)[0], angle=13*j, mode= 'wrap')), 1))\n",
    "\n",
    "rotated_images = []\n",
    "for i in mel.nonzero()[0]:\n",
    "    for j in range(1, 26): \n",
    "        rotated_images.append((torch.tensor(rotate(dataloader.dataset.__getitem__(i)[0], angle=13*j, mode= 'wrap')), 1))\n",
    "    \n",
    "final_images = dataset.__add__(rotated_images)\n",
    "dataloader = torch.utils.data.DataLoader(final_images, batch_size=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected NN and ConvNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3*224*224\n",
    "output_size = 2\n",
    "\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class leaky_CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "def train(epoch, model, perm = torch.arange(0,150528).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target, img_id) in enumerate(dataloader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))\n",
    "            \n",
    "def test(model, perm = torch.arange(0,150528).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_auc_list = []\n",
    "    for data, target, img_id in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        \n",
    "        output_array = np.array([np.max(output.detach().numpy()[i]) for i in range(output.size()[0])])\n",
    "        fpr, tpr, _ = roc_curve(target.detach().numpy(), output_array)\n",
    "        test_auc_list.append(auc(fpr, tpr))\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)   AUC: ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy, np.array(test_auc_list).mean() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1204322\n",
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.936367\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.000160\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.000059\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.000415\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.000402\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [400/2000 (20%)]\tLoss: 0.000039\n",
      "Train Epoch: 1 [800/2000 (40%)]\tLoss: 0.000202\n",
      "Train Epoch: 1 [1200/2000 (60%)]\tLoss: 0.000018\n",
      "Train Epoch: 1 [1600/2000 (80%)]\tLoss: 0.000127\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 2 [0/2000 (0%)]\tLoss: 0.000024\n",
      "Train Epoch: 2 [400/2000 (20%)]\tLoss: 0.000023\n",
      "Train Epoch: 2 [800/2000 (40%)]\tLoss: 0.000007\n",
      "Train Epoch: 2 [1200/2000 (60%)]\tLoss: 0.000033\n",
      "Train Epoch: 2 [1600/2000 (80%)]\tLoss: 0.000256\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fully connected network\n",
    "\n",
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1125968\n",
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.693506\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.600084\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.519524\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.453838\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.399861\n",
      "\n",
      "Test set: Average loss: 0.3550, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 0.355048\n",
      "Train Epoch: 1 [400/2000 (20%)]\tLoss: 0.317492\n",
      "Train Epoch: 1 [800/2000 (40%)]\tLoss: 0.285733\n",
      "Train Epoch: 1 [1200/2000 (60%)]\tLoss: 0.258664\n",
      "Train Epoch: 1 [1600/2000 (80%)]\tLoss: 0.235380\n",
      "\n",
      "Test set: Average loss: 0.2152, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 2 [0/2000 (0%)]\tLoss: 0.215221\n",
      "Train Epoch: 2 [400/2000 (20%)]\tLoss: 0.197640\n",
      "Train Epoch: 2 [800/2000 (40%)]\tLoss: 0.182217\n",
      "Train Epoch: 2 [1200/2000 (60%)]\tLoss: 0.168589\n",
      "Train Epoch: 2 [1600/2000 (80%)]\tLoss: 0.156504\n",
      "\n",
      "Test set: Average loss: 0.1457, Accuracy: 2000/2000 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ConvNet\n",
    "# Training settings \n",
    "n_features = 8 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky CNN (Can do this last / skip if no time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNet with Leaky ReLU Activation Function\n",
    "# Training settings \n",
    "n_features = 8 # number of feature maps\n",
    "\n",
    "model_cnn = leakyCNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "As a preprocessing step, we will try to run the images through an autoencoder to reduce image noise and use the outputs as inputs for the CNN. This will hopefully lead to increased classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define AutoEncoder Class\n",
    "input_size = 3*224*224\n",
    "output_size = 2\n",
    "d = 500\n",
    "n_feature = 8\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, d),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "class leaky_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, d),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.716526\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.000000\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "AE_model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "perm = torch.arange(0,150528).long()\n",
    "\n",
    "### Configure the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    AE_model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "img_ids = []\n",
    "prob_list = []\n",
    "\n",
    "AE_model.train()\n",
    "for batch_idx, (data, target, img_id) in enumerate(dataloader):\n",
    "    img_ids.append(img_id)\n",
    "    # send to device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # permute pixels\n",
    "    data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = AE_model(data.view(-1, 3*224*224))\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    prob_list.append(np.array([i[1]/i.sum() for i in np.abs(output.detach().numpy())]))\n",
    "    if batch_idx % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "            100. * batch_idx / len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmeng1/opt/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:813: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 2000/2000 (100%)   AUC: (nan%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AE_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "test_auc_list = []\n",
    "\n",
    "img_ids = []\n",
    "prob_list = []\n",
    "\n",
    "for data, target, img_id in test_loader:\n",
    "    img_ids.append(img_id)\n",
    "    # send to device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # permute pixels\n",
    "    data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "    output = AE_model(data.view(-1, 3*224*224))\n",
    "    test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "\n",
    "#     output_array = np.array([np.max(output.detach().numpy()[i]) for i in range(output.size()[0])])\n",
    "    fpr, tpr, _ = roc_curve(target.detach().numpy(), output_array)\n",
    "    test_auc_list.append(auc(fpr, tpr))\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    prob_list.append(np.array([i[1]/i.sum() for i in np.abs(output.detach().numpy())]))\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "accuracy_list.append(accuracy)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)   AUC: ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    accuracy, np.array(test_auc_list).mean() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Leaky AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.622717\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.000000\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "img_ids = []\n",
    "prob_list = []\n",
    "\n",
    "AE_model = leaky_Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "### Configure the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    AE_model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "\n",
    "AE_model.train()\n",
    "for batch_idx, (data, target, img_id) in enumerate(dataloader):\n",
    "    img_ids.append(img_id)\n",
    "    # send to device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # permute pixels\n",
    "    data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = AE_model(data.view(-1, 3*224*224))\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    prob_list.append(np.array([i[1]/i.sum() for i in np.abs(output.detach().numpy())]))\n",
    "    if batch_idx % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "            100. * batch_idx / len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Leaky AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "\n",
    "### Code by Andrew Jong https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0][-16:-4]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "dataset = ImageFolderWithPaths(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/train_folder\", transform=transform)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()])\n",
    "#dataset = datasets.ImageFolder(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/train_folder\", transform=transform)\n",
    "test_dataset = ImageFolderWithPaths(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/test_folder\", transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, shuffle=True)\n",
    "\n",
    "mel = np.array(dataset.targets) == 1\n",
    "\n",
    "# for i in mel.nonzero()[0]:\n",
    "#     rotated_images_batch = []\n",
    "#     for j in range(1, 41): # range depends on batch size\n",
    "#         rotated_images_batch.append((torch.tensor(rotate(dataloader.dataset.__getitem__(i)[0], angle=13*j, mode= 'wrap')), 1))\n",
    "\n",
    "rotated_images = []\n",
    "for i in mel.nonzero()[0]:\n",
    "    for j in range(1, 26): \n",
    "        rotated_images.append((torch.tensor(rotate(dataloader.dataset.__getitem__(i)[0], angle=13*j, mode= 'wrap')), 1))\n",
    "    \n",
    "final_images = dataset.__add__(rotated_images)\n",
    "dataloader = torch.utils.data.DataLoader(final_images, batch_size=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmeng1/opt/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:813: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 2000/2000 (100%)   AUC: (nan%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_ids = []\n",
    "prob_list = []\n",
    "\n",
    "AE_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "test_auc_list = []\n",
    "for data, target, img_id in test_loader:\n",
    "    img_ids.append(img_id)\n",
    "    # send to device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # permute pixels\n",
    "    data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "    output = AE_model(data.view(-1, 3*224*224))\n",
    "    test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "\n",
    "    output_array = np.array([np.max(output.detach().numpy()[i]) for i in range(output.size()[0])])\n",
    "    fpr, tpr, _ = roc_curve(target.detach().numpy(), output_array)\n",
    "    test_auc_list.append(auc(fpr, tpr))\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    prob_list.append(np.array([i[1]/i.sum() for i in np.abs(output.detach().numpy())]))\n",
    "    \n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "accuracy_list.append(accuracy)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)   AUC: ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    accuracy, np.array(test_auc_list).mean() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-479de366d73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkaggle_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"img_id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mel_prob\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkaggle_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
     ]
    }
   ],
   "source": [
    "kaggle_submission = pd.DataFrame({\"img_id\":np.concatenate(img_ids), \"mel_prob\":np.concatenate(prob_list)})\n",
    "kaggle_submission.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
