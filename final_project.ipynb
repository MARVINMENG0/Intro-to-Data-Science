{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_model, set_default\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from skimage.transform import rotate\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import gaussian\n",
    "from skimage.io import imread, imsave\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from torchsummary import summary\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code adapted from Yann LeCun and Alfredo Canziani 2019 Spring NYU Deep Learning Course\n",
    "set_default()\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown                               27124\n",
       "nevus                                  5193\n",
       "melanoma                                584\n",
       "seborrheic keratosis                    135\n",
       "lentigo NOS                              44\n",
       "lichenoid keratosis                      37\n",
       "solar lentigo                             7\n",
       "cafe-au-lait macule                       1\n",
       "atypical melanocytic proliferation        1\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "data.set_index(\"patient_id\", inplace=True)\n",
    "data[\"diagnosis\"].value_counts()\n",
    "# data[data[\"diagnosis\"] == \"melanoma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/train_folder\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(\"/Users/jinmeng1/Desktop/College/Grad School/First Year Masters/Fall Semester/Intro to Data Science/Final/images/test_folder\", transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, shuffle=True)\n",
    "\n",
    "# images, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "Rotating melanoma images to number of samples with melanoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = np.array(dataset.targets) == 1\n",
    "\n",
    "# for i in mel.nonzero()[0]:\n",
    "#     rotated_images_batch = []\n",
    "#     for j in range(1, 41): # range depends on batch size\n",
    "#         rotated_images_batch.append((torch.tensor(rotate(dataloader.dataset.__getitem__(i)[0], angle=13*j, mode= 'wrap')), 1))\n",
    "\n",
    "rotated_images = []\n",
    "for i in mel.nonzero()[0]:\n",
    "    for j in range(1, 26): \n",
    "        rotated_images.append((torch.tensor(rotate(dataloader.dataset.__getitem__(i)[0], angle=13*j, mode= 'wrap')), 1))\n",
    "    \n",
    "final_images = dataset.__add__(rotated_images)\n",
    "dataloader = torch.utils.data.DataLoader(final_images, batch_size=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = mpimg.imread(dataloader.dataset.imgs[0][0])\n",
    "\n",
    "# # rotated = rotate(image, angle= 45, mode='wrap')\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=10, figsize=(20,20))\n",
    "\n",
    "# # ax[0].imshow(image)\n",
    "# # ax[1].imshow(rotated)\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for i in range(10):\n",
    "#     data.append(rotate(image, angle=10*i, mode= 'wrap'))\n",
    "\n",
    "\n",
    "# # rotated = rotate(dataloader.dataset.__getitem__(1)[0], angle=45, mode= 'wrap')\n",
    "# # # rotated2 = rotate(dataloader.dataset.__getitem__(1)[0], angle=45, mode= 'wrap')\n",
    "\n",
    "# for i in range(10):\n",
    "#     ax[i].imshow(data[i])\n",
    "\n",
    "\n",
    "# # # np.unique(np.array(dataset.targets))\n",
    "# # # np.unique(np.array(test_dataset.targets))\n",
    "# # # thing = dataloader.dataset.__getitem__(1)[0]\n",
    "# # # thing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected NN and ConvNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3*224*224\n",
    "output_size = 2\n",
    "\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "#         print(x.size())\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "def train(epoch, model, perm = torch.arange(0,150528).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#         data = dataloader.dataset.__getitem__(0)[0].view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#         data = data.view(-1, 28*28)\n",
    "#         data = data[:, perm]\n",
    "#         data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))\n",
    "            \n",
    "def test(model, perm = torch.arange(0,150528).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_auc_list = []\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#        data = dataloader.dataset.__getitem__(0)[0].view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "#         data = data.view(-1, 28*28)\n",
    "#         data = data[:, perm]\n",
    "#         data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        \n",
    "        output_array = np.array([np.max(output.detach().numpy()[i]) for i in range(output.size()[0])])\n",
    "        fpr, tpr, _ = roc_curve(target.detach().numpy(), output_array)\n",
    "        test_auc_list.append(auc(fpr, tpr))\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)   AUC: ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy, np.array(test_auc_list).mean() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1125968\n",
      "Train Epoch: 0 [0/2050 (0%)]\tLoss: 0.679463\n",
      "Train Epoch: 0 [400/2050 (19%)]\tLoss: 0.031422\n",
      "Train Epoch: 0 [800/2050 (38%)]\tLoss: 0.110958\n",
      "Train Epoch: 0 [1200/2050 (58%)]\tLoss: 0.102519\n",
      "Train Epoch: 0 [1600/2050 (77%)]\tLoss: 0.007114\n",
      "Train Epoch: 0 [2000/2050 (96%)]\tLoss: 0.402176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmeng1/opt/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:813: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2626, Accuracy: 2000/2000 (100%)   AUC: (nan%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ConvNet\n",
    "# Training settings \n",
    "n_features = 8 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1204322\n",
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.936367\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.000160\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.000059\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.000415\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.000402\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [400/2000 (20%)]\tLoss: 0.000039\n",
      "Train Epoch: 1 [800/2000 (40%)]\tLoss: 0.000202\n",
      "Train Epoch: 1 [1200/2000 (60%)]\tLoss: 0.000018\n",
      "Train Epoch: 1 [1600/2000 (80%)]\tLoss: 0.000127\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 2 [0/2000 (0%)]\tLoss: 0.000024\n",
      "Train Epoch: 2 [400/2000 (20%)]\tLoss: 0.000023\n",
      "Train Epoch: 2 [800/2000 (40%)]\tLoss: 0.000007\n",
      "Train Epoch: 2 [1200/2000 (60%)]\tLoss: 0.000033\n",
      "Train Epoch: 2 [1600/2000 (80%)]\tLoss: 0.000256\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 2000/2000 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fully connected network\n",
    "\n",
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1125968\n",
      "Train Epoch: 0 [0/2000 (0%)]\tLoss: 0.693506\n",
      "Train Epoch: 0 [400/2000 (20%)]\tLoss: 0.600084\n",
      "Train Epoch: 0 [800/2000 (40%)]\tLoss: 0.519524\n",
      "Train Epoch: 0 [1200/2000 (60%)]\tLoss: 0.453838\n",
      "Train Epoch: 0 [1600/2000 (80%)]\tLoss: 0.399861\n",
      "\n",
      "Test set: Average loss: 0.3550, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 1 [0/2000 (0%)]\tLoss: 0.355048\n",
      "Train Epoch: 1 [400/2000 (20%)]\tLoss: 0.317492\n",
      "Train Epoch: 1 [800/2000 (40%)]\tLoss: 0.285733\n",
      "Train Epoch: 1 [1200/2000 (60%)]\tLoss: 0.258664\n",
      "Train Epoch: 1 [1600/2000 (80%)]\tLoss: 0.235380\n",
      "\n",
      "Test set: Average loss: 0.2152, Accuracy: 2000/2000 (100%)\n",
      "\n",
      "Train Epoch: 2 [0/2000 (0%)]\tLoss: 0.215221\n",
      "Train Epoch: 2 [400/2000 (20%)]\tLoss: 0.197640\n",
      "Train Epoch: 2 [800/2000 (40%)]\tLoss: 0.182217\n",
      "Train Epoch: 2 [1200/2000 (60%)]\tLoss: 0.168589\n",
      "Train Epoch: 2 [1600/2000 (80%)]\tLoss: 0.156504\n",
      "\n",
      "Test set: Average loss: 0.1457, Accuracy: 2000/2000 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ConvNet\n",
    "# Training settings \n",
    "n_features = 8 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "As a preprocessing step, we will try to run the images through an autoencoder to reduce image noise and use the outputs as inputs for the CNN. This will hopefully lead to increased classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define AutoEncoder Class\n",
    "input_size = 3*224*224\n",
    "output_size = 2\n",
    "d = 500\n",
    "n_feature = 8\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, d),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=n_features, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(n_features, n_features, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(self.n_feature*53*53, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, 3*224*224)[:, perm].view(-1,3,224,224)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "#         print(x.size())\n",
    "        x = x.view(-1, self.n_feature*53*53)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# AE_model = Autoencoder().to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Define model architecture and reconstruction loss\n",
    "\n",
    "# n = 28 x 28 = 784\n",
    "# d = 30  # for standard AE (under-complete hidden layer)\n",
    "# d = 500  # for denoising AE (over-complete hidden layer)\n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(input_size, d),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(d, input_size),\n",
    "#             nn.Tanh(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x\n",
    "    \n",
    "AE_model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    AE_model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinmeng1/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([40, 150528])) that is different to the input size (torch.Size([40, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (150528) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-7bc25652713b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# ===================forward=====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bad\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# feed <img> (for std AE) or <img_bad> (for denoising AE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# ===================backward====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (150528) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# for data in dataloader:\n",
    "#     img, _ = data\n",
    "#     img = img.to(device)\n",
    "#     img = img.view(img.size(0), -1)\n",
    "#     noise = do(torch.ones(img.shape))\n",
    "#     img_bad = (img * noise).to(device)\n",
    "#     print(img.shape, noise.shape, img_bad.shape)\n",
    "\n",
    "# Train standard or denoising autoencoder (AE)\n",
    "\n",
    "perm = torch.arange(0,150528).long()\n",
    "\n",
    "num_epochs = 1\n",
    "do = nn.Dropout()  # comment out for standard AE\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "        img.requires_grad_()\n",
    "        img = img.view(img.size(0), -1)\n",
    "        noise = do(torch.ones(img.shape))\n",
    "        img_bad = (img * noise).to(device)  # comment out for standard AE\n",
    "        # ===================forward=====================\n",
    "        output = AE_model(img_bad)  # feed <img> (for std AE) or <img_bad> (for denoising AE)\n",
    "        loss = criterion(output, img.data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print(f'epoch [{epoch + 1}/{num_epochs}], loss:{loss.item():.4f}')\n",
    "#     display_images(None, output)  # pass (None, output) for std AE, (img_bad, output) for denoising AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150528"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
